#+TITLE: Polymorphism
#+AUTHOR: <lngnmn2@yahoo.com>

There is something very deep and profound behind the abstract notion of so-called /currying/ (after Haskell B. Curry).

The trick, William Poter, is to think of a multiargument functions as being "a syntactic sugar" for a bunch of /nested/ lambdas (closures), each of which /captures its single argument/ and return another lambda (closure).

#+BEGIN_SRC scheme
(define plus (lambda (x)
               (lambda (y)
                 (+ x y))))

;; application is "explicit"
((plus 2) 3)
#+END_SRC
It is not a random coincidence that both definition and application expressions are explicitly /nested/.

Notice that there are always two distinct aspects -- a mathematical abstraction (on paper) and its representation (and implementation) inside a computer (in terms of some programming language).

In /Haskell/ this is just
#+BEGIN_SRC haskell
> let plus = \x -> \y -> x + y
> plus 2 3
#+END_SRC
In classic languages (such as /ML/, /Haskell/ and /Ocaml/) a multiargument function (both definition and application) is just a /syntactic sugar/, which "desugars" into a /nested lambdas/ expression. So, in a pseudo-Haskell:
~\x y ->~ becomes ~\x -> \y ->~ and ~f x y~ becomes ~(f x) y~.

It is not a random coincidence that the /type signature/ looks like a path on a graph ~a -> a -> a~:
#+BEGIN_SRC haskell
> :t plus
plus :: Num a => a -> a -> a
#+END_SRC
and as "nested" /implications/. These are just different abstract notions are of the same underlying universal phenomena.

Another "universal" notion of /Partial Application/ comes "naturally" -- one just applies a function to a fewer arguments, just like a /partially filled enzyme/ (which performs its "action" only when all necessary and sufficient "slots" are filled with particular molecular structures, usually /in any order/, or strictly in a particular order, which means that it will "wait" for a certain molecule to bind and potentially transform the shape of the enzyme in such a way that another "slot" will open). These things are so deep...

In /Haskell/, just like in math, /any/ function can be partially applied.
#+BEGIN_SRC haskell
> :t plus 2
plus 2 :: Num a => a -> a
> (plus 2) 3
5
#+END_SRC
This is not just a "trick" or a "hack". A /partially applied/ ~map~ /"crosses an abstraction barrier"/ and yields a function /on lists/.
#+BEGIN_SRC haskell
> :t map (+1)
map (+1) :: Num b => [b] -> [b]
#+END_SRC
and more generally
#+BEGIN_SRC haskell
> :t fmap (+1)
map (+1) :: (Functor f, Num b) => f b -> f b
#+END_SRC
Switch the order of arguments of ~fmap~, and then partially apply it to a particular "context" or an abstract "container", and one will have a function which expects a "transformation" (a function) and returns a modified "context" or a "container".
#+BEGIN_SRC haskell
> :t flip fmap
flip fmap :: Functor f => f a -> (a -> b) -> f b
#+END_SRC

Add the Functor laws (associativity of composition and left and right identities) and you will have essentially this:
#+BEGIN_SRC haskell
> :t (>>=)
(>>=) :: Monad m => m a -> (a -> m b) -> m b
#+END_SRC
Yes, there are lots of implementation details, such that a ~Monad~ is defined as a /type-class/, which defines an "composition operator" and an "arrow":
#+BEGIN_SRC haskell
> :t return
return :: Monad m => a -> m a
#+END_SRC
and this particular kind of a /"Kleisli arrow"/ is what the ~(>>=)~ requires.

The main point is that with just /nesting (function composition)/ and /partial application/ could yield something bigger than its parts.

Unfortunately, /Scheme/ does not support partial application, but one can use another /wrapper/ function, so /any/ function could be /partially applied/ (at least once).
#+BEGIN_SRC scheme
(define partially (lambda f . xs)
  (lambda ys
    (apply f (append xs ys)))
#+END_SRC

Now look.

Traditionally, in some languages /partial application/ is expressed syntactically just as:
#+BEGIN_SRC scala
xs.map(f)
#+END_SRC
The semantics is exactly as ~(map f) xs~, which, again, is "given just ~f~ a /closure/, which captured ~f~, is returned" and it is being applied to ~xs~".

So, what if the first partial application is not of a /formal parameter/, but of a /type/ for an /"implicit"/ type-parameter (type-variable)?

Suddenly, expressions like
#+BEGIN_SRC rust
fn min<T: Ord>(x: T, y: T) -> T {}
#+END_SRC
begin to look as what they really are.

Types have their own types (or the universe of types is being /partitioned/ according to particular "such that" /constraints/), and a /"partially applied to a particular type"/ genetic function /"yields a concrete function on that type"/.

These "such that constraints" is what a /type-class/ (or a trait) defines /for types/, plus some additional rules or "laws" defined informally somewhere else.

Of course, in /Haskell/ "parameterized type-constructors" can be /partially applied/ to types to yield another type-constructors (which when fully saturated will yield a "concrete type").

This is what a proper parametric polymorphism with type-classes (or traits) looks like. Notice that there is nothing "arbitrary or random" out there. The very same universal notions (from the Lambda Calculus), just at a type level.

And this is where we (the scholars of the classic languages) currently are.
